# Gerenciando um Pipeline de Dados com Apache Airflow

## üí°Objetivos
Desenvolver um pipeline de dados automatizado dedicado √† obten√ß√£o da previs√£o do tempo para a cidade de Boston nos pr√≥ximos 7 dias. O pipeline ser√° respons√°vel por extrair essas informa√ß√µes de uma API confi√°vel, armazenando-as de maneira eficiente e programada para execu√ß√£o regular em um dia determinado. O prop√≥sito principal dessa iniciativa √© fornecer √† cidade de Boston dados precisos e atualizados sobre as condi√ß√µes clim√°ticas futuras, permitindo que ela organize a programa√ß√£o de passeios de maneira mais informada e adequada √†s condi√ß√µes meteorol√≥gicas esperadas.


## üñ•Ô∏èDesafios do Projeto
Este projeto consiste em aprofundar os conhecimentos sobre Apache Airflow, poderosa ferramenta de orquestra√ß√£o de fluxos, visando entender seus conceitos fundamentais, componentes arquitet√¥nicos e recursos essenciais. Instalar o Airflow, explorar sua interface e compreender o papel crucial dos conceitos como DAGs, tasks e operators na cria√ß√£o, agendamento e monitoramento de pipelines de dados e fluxos de trabalho de forma program√°tica.

###### Imagem 1: Arquitetura do Airflow (Cr√©dito da imagem: Alura)
<img src="/assets/img/img_arquitetura_dag.png">

## üìÑDesenvolvimento de Compet√™ncias:
|Atividades|Realizadas |
|----------|-----------|
| Utilizar uma API com dados clim√°ticos | Extrair dados da previs√£o do tempo |
| Criar uma pasta utilizando Python | Salvar os arquivos extra√≠dos em csv |
| Diferenciar DAGs, Tasks e Operators | Listar quais s√£o os principais componentes do Airflow |
| Arquitetura do Airflow | Instalar o Python 3.9 no Ubuntu |
| Criar e ativar um ambiente virtual | Instalar o Airflow |
| Executar o Airflow localmente | Navegar pela interface do Airflow |
| Criar seu primeiro DAG | Instanciar tarefas utilizando o EmptyOperator |
| Desenvolver tarefas utilizando o BashOperator | Utilizar os Jinja Templates |
| Criar um DAG mais complexo | Utilizar as CRON Expressions para definir um intervalo de agendamento |
| Criar tarefas utilizando o PythonOperator | Desenvolver um DAG que extrai dados da previs√£o do tempo |

## üóÇÔ∏èOrganiza√ß√£o dos Arquivos
- Pasta dags: A pasta de DAGs deve ser denominada como "dags" para que o Apache Airflow possa identificar adequadamente o local onde estamos criando esses fluxos de trabalho. √â nesse diret√≥rio espec√≠fico que os arquivos contendo o desenvolvimento de cada DAG devem ser salvos;
- Pasta semana: √â um diret√≥rio semanal criado por meio de um DAG para armazenar os dados extra√≠dos da previs√£o do tempo correspondentes √† semana de execu√ß√£o.

## üéûÔ∏èImagens do Projeto

###### Imagem 2: Inicializa√ß√£o Apache Airflow
<img src="/assets/img/img_start_airflow.png">

###### Imagem 3: Interface Airflow
<img src="/assets/img/img_dag.png">

###### Imagem 4: Visualiza√ß√£o gr√°fica de um DAG e suas depend√™ncias
<img src="/assets/img/img_graph.png">


## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)
